import torch
import pickle
import numpy as np
from tqdm import tqdm
from torch.utils.data import Dataset

class AISTPPG1Dataset(Dataset):
    """
    Dataset for `data/pkls/aistpp_g1.pkl` generated by `data/preprocess_aistpp.py`.

    Each motion_item contains:
      - dof_pos: (T, 36)
      - audio_feat: (T, 4800)
      - fps: float
    We train diffusion directly on dof vectors (no FK / no skeleton).
    """

    def __init__(self, pkl_path, offset_frame, past_frame, future_frame, dtype=np.float32, limited_num=-1):
        if future_frame <= 0:
            raise ValueError(f'future_frame must be > 0, got {future_frame}')
        if past_frame < 0:
            raise ValueError(f'past_frame must be >= 0, got {past_frame}')
        self.pkl_path, self.dtype = pkl_path, dtype
        self.past_frame = past_frame
        self.future_frame = future_frame
        window_size = past_frame + future_frame
        self.reference_frame_idx = past_frame

        data_source = pickle.load(open(pkl_path, 'rb'))
        motions = data_source['motions'][:limited_num]

        self.dof_list = []
        self.audio_list = []
        frame_nums = []
        item_frame_indices_list = []
        motion_idx = 0

        for motion_item in tqdm(motions):
            dof = motion_item['dof_pos'].astype(dtype)          # (T, 36)
            audio = motion_item['audio_feat'].astype(dtype)     # (T, 4800)
            frame_num = dof.shape[0]
            if frame_num < window_size:
                continue
            if audio.shape[0] != frame_num:
                raise RuntimeError(
                    f"Frame mismatch in {motion_item.get('filepath', '<unknown>')}: dof={frame_num}, audio={audio.shape[0]}"
                )

            frame_nums.append(frame_num)
            self.dof_list.append(dof)
            self.audio_list.append(audio)

            clip_indices = np.arange(0, frame_num - window_size + 1, offset_frame)[:, None] + np.arange(window_size)
            clip_indices_with_idx = np.hstack((np.full((len(clip_indices), 1), motion_idx, dtype=clip_indices.dtype), clip_indices))
            item_frame_indices_list.append(clip_indices_with_idx)
            motion_idx += 1

        self.item_frame_indices = np.concatenate(item_frame_indices_list, axis=0)

        # expose properties compatible with train.py expectations
        self.dof_dim = int(self.dof_list[0].shape[-1])
        self.audio_dim = int(self.audio_list[0].shape[-1])
        self.joint_num = 1
        self.per_rot_feat = self.dof_dim
        self.input_feats = self.joint_num * self.per_rot_feat
        self.style_set = [0]  # dummy single style
        self.mask = np.ones(future_frame, dtype=bool)

        print(
            'AISTPPG1Dataset loaded, trained with %d clips, %d frames, %d mins in total'
            % (len(frame_nums), sum(frame_nums), sum(frame_nums) / 30 / 60)
        )

    def __len__(self):
        return len(self.item_frame_indices)

    def __getitem__(self, idx):
        item_frame_indice = self.item_frame_indices[idx]
        motion_idx, frame_indices = item_frame_indice[0], item_frame_indice[1:]

        dof = self.dof_list[motion_idx][frame_indices].copy()      # (window, dof_dim)
        audio = self.audio_list[motion_idx][frame_indices].copy()  # (window, audio_dim)

        past_dof = dof[: self.reference_frame_idx]
        future_dof = dof[self.reference_frame_idx :]

        future_audio = audio[self.reference_frame_idx :]  # align with future frames

        # model expects [bs, joints, feats, frames] inside portal after permute;
        # dataset returns per-sample tensors and dataloader will stack on batch dim.
        past_motion = torch.from_numpy(past_dof[:, None, :])    # (past, 1, dof_dim)
        future_motion = torch.from_numpy(future_dof[:, None, :])  # (future, 1, dof_dim)
        audio_feat = torch.from_numpy(future_audio.T)  # (audio_dim, future)

        return {
            'data': future_motion,
            'conditions': {
                'past_motion': past_motion,
                'audio_feat': audio_feat,
                'mask': torch.from_numpy(self.mask.copy())
            }
        }